# Рекурентные НС - уже не дети
# Они знают, что будет на шаг вперед
# Они учитывают опыт поколений
# Они хороши для временных последовательностей: 
# - звуковых сигналов
# - положения тела в пространстве
# - текстовой информации
# Посмотрим, как РНС справится с задачей, если на вход ей подавать несколько символов
# и требовать с нее предсказать следующий

import numpy as np
import re
from tensorflow.keras.layers import Dense, SimpleRNN, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
import pathlib
from pathlib import Path

# разрешим ноутбуку доступ к гугл-диску если выполняемся в Google Colab
from google.colab import drive
drive.mount('/content/gdrive')

# загрузим текст, на котором будет обучаться НС
with open('/content/gdrive/MyDrive/Colab Notebooks/text.txt', 'r', encoding='utf-8') as f:
    text = f.read()
    text = text.replace('\ufeff', '')  # убираем первый невидимый символ
    text = re.sub(r'[^А-я ]', '', text) # убираем все символы, кроме русских букв и пробела
    
# спарсим текст как последовательность символов
num_characters = 34 # 33 буквы и пробел
tokenizer = Tokenizer(num_words=num_characters, char_level=True)  # токенизируем на уровне символов
tokenizer.fit_on_texts([text])  # формируем токены на основе частотности в нашем тексте
print(tokenizer.word_index)
# увидим нумерацию символов (не по порядку) - пригодится при переводе прогноза в конкретный символ

inp_chars = 6
data = tokenizer.texts_to_matrix(text)  # преобразуем исходный текст в массив OHE
n = data.shape[0] - inp_chars  # так как мы предсказываем по трем символам - четвертый

# на основе OHE-векторов сделаем обучающую выборку
# берем inp_chars, следующий предсказываем, затем сдвигаем и повторяем
# сформируем выборку с помощью list-comprehention

X = np.array([data[i:i + inp_chars, :] for i in range(n)])
Y = data[inp_chars:]  # предсказание следующего символа

print(data.shape)

# построим РНС

model = Sequential()
# входной слой
# формат: (batch_size/inp_chars/num_characters)
# размер батча/число символов, на основе которых будет прогноз/размер OHE векторов (длина вектора по количеству символов)
model.add(Input((inp_chars,
                 num_characters)))  # при тренировке в рекуррентные модели keras подается сразу вся последовательность, поэтому в input теперь два числа. 1-длина последовательности, 2-размер OHE
# как обычный Dense, только рекурентный
# по умолчанию активация-гиперб.тангенс
model.add(SimpleRNN(128, activation='tanh'))  # рекуррентный слой на 500 нейронов
# полносвязный слой на выходе
model.add(Dense(num_characters, activation='softmax'))
model.summary()


# скомпилируем НС
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
history=model.fit(X, Y, batch_size=32, epochs=100)

# функция построения фразы на основе прогнозных значений
def buildPhrase(inp_str, str_len=50): # на вход начальные символы
    # формирование начальных символов в формате OHE
    for i in range(str_len):
        x = []
        for j in range(i, i + inp_chars):
            x.append(tokenizer.texts_to_matrix(inp_str[j]))  # преобразуем символы в One-Hot-encoding

        x = np.array(x)
        inp = x.reshape(1, inp_chars, num_characters) 

        pred = model.predict(inp)  # предсказываем OHE четвертого символа
        # выбираем индекс наибольшего значения вероятности
        # ищем в коллекции соответствие индекса символу
        d = tokenizer.index_word[pred.argmax(axis=1)[0]]  # получаем ответ в символьном представлении

        inp_str += d  # дописываем строку
    return inp_str

# позволим сети немного побредить
res = buildPhrase("подъезд")
print(res)
