# простой перевод градусов Цельсия в градусы Фаренгейта
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras.layers import Dense

# Обучающая выборка
c = np.array([-40, -10,  0,  8, 15, 22, 38]) # градусы цельсия
f = np.array([-40,  14, 32, 46, 59, 72, 100]) # градусы фаренгейта - ожидаемые выходы

model = keras.Sequential() #модель многослойной нейронной сети - последовательноси слоев
# класс Dense создает слой нейронов полносвязной нейронной сети
# кол-во нейронов / кол-во входов (bias не в счет) / какая активационная ф-ия
model.add(Dense(units=1, input_shape = (1,), activation='linear'))
# теперь, когда структура нейронной сеи определена, ее надо скомпилировать, 
# указав критерий качества и способ оптимизации градиентного спуска
# выберем минимум среднего квадрата ошибки и оптимизацию по Adam
model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(0.1)) # 0.1 - шаг сходимости градиентного спуска
# после вызова метода compile весовые коэф-ты принимают случайные начальные значения

# сеть готова к обучению

# запуск алгоритма обучения производится следующим методом:
# входные значения / ожидаемые выходные / число эпох / вывод в консоль служебной информации
history = model.fit(c, f, epochs=500, verbose = False)

# график по словарю history, содержащему значение критерия качества (в данном случае mean_squared_error) для каждой из эпох
plt.plot(history.history['loss'])
plt.grid(True)
plt.show()

# тестируем
print(model.predict([100]))
# видим вот это: [[211.28404]]
# близко, не так ли?

# возвратим весовые коэф-ты нейронной сети
print(model.get_weights())

# видим коэффициенты линейной модели
# [array([[1.8279082]], dtype=float32), array([28.493229], dtype=float32)]
