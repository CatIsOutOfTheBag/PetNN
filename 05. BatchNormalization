# Статистическая картина меняется внутри сети от слоя к слою.
# Это снижает скорость обучения и приходится делать малый шаг сходимости
# Решением является внутренний ковариационный сдвиг - нормализация с мат.ожиданием=0 и дисперсией=1
# Но разницу стат.характеристик нужно сохранять и распространять по сети,
# в то время как нулевое мат ожидание приводит нас в такую точку на сигмоиде, где 
# функция активации близка к линейной
# а нам нельзя терять нелинейность
# Поэтому нормализованные значения векторов дополнительно масштабируются и сдвигаются
# y = gamma*z + betta
# где гамма и бетта так же подбираются градиентным спуском

# Внимание!
# По-хорошему, НС нужно строить так, чтобы не пригодились никакие DropOut и BatchNorm


# Вот тут посмотрим как поведет себя простая НС  c BatchNormalization
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization
from sklearn.model_selection import train_test_split

(X_train, y_train), (X_test, y_test) = mnist.load_data()
# нормализация входных данных, переход в [0,1]
X_train = X_train / 255 # 255 - максимальное значени
X_test = X_test / 255

# Преобразование выходных значений в векторы по категориям - в вектор 0 и 1
# 5 -> 0000010000
y_train_cat = keras.utils.to_categorical(y_train,10)
y_test_cat = keras.utils.to_categorical(y_test,10)

# в ручную разобьем выборку на обучающую и валидационную
size_val = 5000 # объем валидационной выборки МАЛЕНЬКИЙ
x_train_data = X_train[:size_val]
y_train_data = y_train_cat[:size_val]

x_valid = X_train[size_val:size_val*2]
y_valid = y_train_cat[size_val:size_val*2]

model = keras.Sequential([
      Flatten(input_shape=(28,28,1)), #1 означает 1 байт-один пиксель
      Dense(3000, activation='relu'), # на скрытом слое МНОГО нейронов
      BatchNormalization(), # после слоя, к которому применяется
      # нормализация будет применена уже к выходам слоя из 300 нейронов
      # норма-ия эффективна при большом числе слоев - в deep learning
      # для простой сети малой глубины эффект может быть негативным
      Dense(10, activation='softmax')                
]) 

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

his = model.fit(x_train_data, y_train_data, batch_size=32, epochs=50, 
          validation_data=(x_valid, y_valid)) # это ручной вариант разбивки
          
plt.plot(his.history['loss'])
plt.plot(his.history['val_loss'])
plt.show()


# А теперь без BatchNormalization

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization
from sklearn.model_selection import train_test_split

(X_train, y_train), (X_test, y_test) = mnist.load_data()
# нормализация входных данных, переход в [0,1]
X_train = X_train / 255 # 255 - максимальное значени
X_test = X_test / 255

# Преобразование выходных значений в векторы по категориям - в вектор 0 и 1
# 5 -> 0000010000
y_train_cat = keras.utils.to_categorical(y_train,10)
y_test_cat = keras.utils.to_categorical(y_test,10)

# в ручную разобьем выборку на обучающую и валидационную
size_val = 5000 # объем валидационной выборки МАЛЕНЬКИЙ
x_train_data = X_train[:size_val]
y_train_data = y_train_cat[:size_val]

x_valid = X_train[size_val:size_val*2]
y_valid = y_train_cat[size_val:size_val*2]

model = keras.Sequential([
      Flatten(input_shape=(28,28,1)), #1 означает 1 байт-один пиксель
      Dense(3000, activation='relu'), # на скрытом слое МНОГО нейронов
      #BatchNormalization(), # после слоя, к которому применяется
      # нормализация будет применена уже к выходам слоя из 300 нейронов
      # норма-ия эффективна при большом числе слоев - в deep learning
      # для простой сети малой глубины эффект может быть негативным
      Dense(10, activation='softmax')                
]) 

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

his = model.fit(x_train_data, y_train_data, batch_size=32, epochs=50, 
          validation_data=(x_valid, y_valid)) # это ручной вариант разбивки
          
plt.plot(his.history['loss'])
plt.plot(his.history['val_loss'])
plt.show()
